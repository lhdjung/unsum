[{"path":"https://lhdjung.github.io/unsum/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 unsum authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://lhdjung.github.io/unsum/articles/unsum.html","id":"closure-results","dir":"Articles","previous_headings":"","what":"CLOSURE results","title":"Get started","text":"Now let’s look results : inputs records arguments closure_generate(). metrics shows number possible samples led reported summary statistics (samples_all) total number values found (values_all). Importantly, also features index variation bounded scales (horns). ranges 0 1, 0 means variability 1 sample evenly split extremes — , 1, 5 — values . reference value horns_uniform shows value horns mean sample uniformly distributed. 0.5 1-5 scale. See horns() details. actual horns value 0.79, high degree variability even abstract. However, practice, 0.79 might extremely high compared theoretical expectations: sample roughly normal shape, even hypothetical 0.5 uniform value surprisingly high, let alone 0.79 actual value. frequency shows absolute relative frequencies values found CLOSURE scale point. also gives us (absolute) frequency values average sample saw plot . results stores samples CLOSURE found (sample). unique number (id). See closure_generate() details. addition bar plot, unsum offers ECDF plot CLOSURE results:","code":"data #> $inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 3.5   1.8      80         1         5 up_or_down         5 #>  #> $metrics #> # A tibble: 1 × 5 #>   samples_initial samples_all values_all horns horns_uniform #>             <int>       <int>      <int> <dbl>         <dbl> #> 1              15        2215     177200 0.792           0.5 #>  #> $frequency #> # A tibble: 5 × 4 #>   value f_average f_absolute f_relative #>   <int>     <dbl>      <int>      <dbl> #> 1     1     22.9       50778     0.287  #> 2     2      5.24      11598     0.0655 #> 3     3      3.81       8439     0.0476 #> 4     4      5.24      11609     0.0655 #> 5     5     42.8       94776     0.535  #>  #> $results #> # A tibble: 2,215 × 2 #>       id sample     #>    <int> <list>     #>  1     1 <int [80]> #>  2     2 <int [80]> #>  3     3 <int [80]> #>  4     4 <int [80]> #>  5     5 <int [80]> #>  6     6 <int [80]> #>  7     7 <int [80]> #>  8     8 <int [80]> #>  9     9 <int [80]> #> 10    10 <int [80]> #> # ℹ 2,205 more rows closure_plot_ecdf(data)"},{"path":"https://lhdjung.github.io/unsum/articles/unsum.html","id":"horns-index-variation","dir":"Articles","previous_headings":"","what":"Horns index variation","title":"Get started","text":"may wonder variability samples. Couldn’t much lower higher horns index overall mean horns? case, chance original data looked quite different average. Check using closure_horns_analyze(): Also, closure_horns_histogram() visualizes distribution horns values across samples:  sum, horns values quite tightly confined. Wide variation among seems occur mean sd decimal places.","code":"data_horns <- closure_horns_analyze(data) data_horns #> $closure_generate_inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 3.5   1.8      80         1         5 up_or_down         5 #>  #> $horns_metrics #> # A tibble: 1 × 9 #>    mean uniform     sd     cv    mad   min median   max  range #>   <dbl>   <dbl>  <dbl>  <dbl>  <dbl> <dbl>  <dbl> <dbl>  <dbl> #> 1 0.792     0.5 0.0251 0.0317 0.0189 0.756  0.787 0.844 0.0877 #>  #> $horns_results #> # A tibble: 2,215 × 2 #>       id horns #>    <int> <dbl> #>  1     1 0.762 #>  2     2 0.756 #>  3     3 0.756 #>  4     4 0.762 #>  5     5 0.756 #>  6     6 0.762 #>  7     7 0.762 #>  8     8 0.762 #>  9     9 0.756 #> 10    10 0.756 #> # ℹ 2,205 more rows closure_horns_histogram(data_horns)"},{"path":"https://lhdjung.github.io/unsum/articles/unsum.html","id":"read-and-write","dir":"Articles","previous_headings":"","what":"Read and write","title":"Get started","text":"huge object CLOSURE results want save? Write disk closure_write(): stores results using highly efficient Parquet format. take tiny fraction CSV file’s disk space. later session, read data folder get CLOSURE list back: caveat: don’t modify output closure_generate() passing closure_*() functions. latter need input specific format, manipulate data two closure_*() calls, assumptions may longer hold. checks place detect alterations, may catch .","code":"# Using a temporary folder via `tempdir()` just for this example -- # you should use a real folder on your computer instead! path_new_folder <- closure_write(data, path = tempdir()) #> ✔ All files written to: #> /tmp/Rtmp6nUBud/CLOSURE-3_5-1_8-80-1-5-up_or_down-5/ data_new <- closure_read(path_new_folder)"},{"path":"https://lhdjung.github.io/unsum/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lukas Jung. Author, maintainer. Nathanael Larigaldie. Contributor.","code":""},{"path":"https://lhdjung.github.io/unsum/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Jung L (2025). unsum: Reconstruct Raw Data Summary Statistics. R package version 0.2.0, https://github.com/lhdjung/unsum.","code":"@Manual{,   title = {unsum: Reconstruct Raw Data from Summary Statistics},   author = {Lukas Jung},   year = {2025},   note = {R package version 0.2.0},   url = {https://github.com/lhdjung/unsum}, }"},{"path":"https://lhdjung.github.io/unsum/index.html","id":"unsum-reconstruct-raw-data-from-summary-statistics","dir":"","previous_headings":"","what":"Reconstruct Raw Data from Summary Statistics","title":"Reconstruct Raw Data from Summary Statistics","text":"goal unsum undo summarization: reconstruct possible samples may underlie given set summary statistics. currently supports sets mean, SD, sample size, scale bounds. can useful forensic metascience identify impossible implausible reported numbers. package features CLOSURE: Complete Listing Original Samples Underlying Raw Evidence, fast algorithm implemented Rust. Go Get started learn use . CLOSURE exhaustive, makes computationally intensive. code takes long run, consider using SPRITE instead (see Previous work ).","code":""},{"path":"https://lhdjung.github.io/unsum/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Reconstruct Raw Data from Summary Statistics","text":"can install unsum either : R version 4.2.0 recent.","code":"remotes::install_github(\"lhdjung/unsum\") # or pak::pkg_install(\"unsum\")"},{"path":"https://lhdjung.github.io/unsum/index.html","id":"demo","dir":"","previous_headings":"","what":"Demo","title":"Reconstruct Raw Data from Summary Statistics","text":"Start closure_generate(), package’s main function. creates possible samples: Visualize overall distribution values found samples:","code":"library(unsum)  data <- closure_generate(   mean = \"2.7\",   sd = \"1.9\",   n = 130,   scale_min = 1,   scale_max = 5 ) #> → Just a second...  data #> $inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 2.7   1.9     130         1         5 up_or_down         5 #>  #> $metrics #> # A tibble: 1 × 5 #>   samples_initial samples_all values_all horns horns_uniform #>             <int>       <int>      <int> <dbl>         <dbl> #> 1              15        5359     696670 0.881           0.5 #>  #> $frequency #> # A tibble: 5 × 4 #>   value f_average f_absolute f_relative #>   <int>     <dbl>      <int>      <dbl> #> 1     1     67.0      358972     0.515  #> 2     2      5.63      30170     0.0433 #> 3     3      4.09      21940     0.0315 #> 4     4      5.63      30162     0.0433 #> 5     5     47.7      255426     0.367  #>  #> $results #> # A tibble: 5,359 × 2 #>       id sample      #>    <int> <list>      #>  1     1 <int [130]> #>  2     2 <int [130]> #>  3     3 <int [130]> #>  4     4 <int [130]> #>  5     5 <int [130]> #>  6     6 <int [130]> #>  7     7 <int [130]> #>  8     8 <int [130]> #>  9     9 <int [130]> #> 10    10 <int [130]> #> # ℹ 5,349 more rows closure_plot_bar(data)"},{"path":"https://lhdjung.github.io/unsum/index.html","id":"previous-work","dir":"","previous_headings":"","what":"Previous work","title":"Reconstruct Raw Data from Summary Statistics","text":"SPRITE generates random datasets led reported statistics. CLOSURE exhaustive, always finds possible datasets, just random sample . reason, SPRITE runs fast CLOSURE may take long. GRIM GRIMMER test reported summary statistics consistency, CLOSURE ultimate consistency test: finds least one distribution, statistics consistent; , correct. CORVIDS deserves credit first technique reconstruct possible underlying datasets. However, takes long run, often prohibitively . partly code written Python, algorithm also inherently much complex CLOSURE.","code":""},{"path":"https://lhdjung.github.io/unsum/index.html","id":"about","dir":"","previous_headings":"","what":"About","title":"Reconstruct Raw Data from Summary Statistics","text":"CLOSURE algorithm originally written Python Nathanael Larigaldie. R package unsum provides easy access optimized implementation Rust, closure-core, via amazing extendr framework. Rust code tends run much faster R Python code, required many applications CLOSURE.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":null,"dir":"Reference","previous_headings":"","what":"Count CLOSURE samples in advance — closure_count_initial","title":"Count CLOSURE samples in advance — closure_count_initial","text":"Determine many samples closure_generate() find given set summary statistics. closure_count_initial() counts first round samples, ones generated. currently closure_count_all() function. can help predict much time closure_generate() take, avoid prohibitively long runs.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count CLOSURE samples in advance — closure_count_initial","text":"","code":"closure_count_initial(scale_min, scale_max)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count CLOSURE samples in advance — closure_count_initial","text":"scale_min, scale_max Integers (length 1 ). Minimum maximum scales reported statistics refer.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count CLOSURE samples in advance — closure_count_initial","text":"Integer (length 1).","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Count CLOSURE samples in advance — closure_count_initial","text":"","code":"closure_count_initial(scale_min = 1, scale_max = 5) #> [1] 15"},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"run closure_generate(), may want get sense time take run. Use closure_gauge_complexity() compute heuristics-based complexity score. reference, determines messages closure_generate():","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"","code":"closure_gauge_complexity(mean, sd, n, scale_min, scale_max)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"mean String (length 1). Reported mean. sd String (length 1). Reported sample standard deviation. n Numeric (length 1). Reported sample size. scale_min, scale_max Numeric (length 1 ). Minimal maximal possible values measurement scale. example, 1-7 Likert scale, use scale_min = 1 scale_max = 7. Prefer empirical min max available: constrain possible values .","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"Numeric (length 1).","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"result function hard interpret. can convey idea likely runtime CLOSURE. input parameters interact highly dynamic ways, makes prediction difficult. addition, even progress bars updates regular intervals (e.g., \"10% complete\") prove extremely challenging: Rust code computes CLOSURE results parallel, makes hard get overview total progress across cores; especially display information R level.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"","code":"# Low SD, N, and scale range: closure_gauge_complexity(   mean = 2.55,   sd = 0.85,   n = 84,   scale_min = 1,   scale_max = 5 ) #> [1] 0.3521825  # Somewhat higher: closure_gauge_complexity(   mean = 4.26,   sd = 1.58,   n = 100,   scale_min = 1,   scale_max = 7 ) #> [1] 2.477121  # Very high: closure_gauge_complexity(   mean = 3.81,   sd = 3.09,   n = 156,   scale_min = 1,   scale_max = 7 ) #> [1] 3.39794"},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate CLOSURE samples — closure_generate","title":"Generate CLOSURE samples — closure_generate","text":"Call closure_generate() run CLOSURE algorithm given set summary statistics. can take seconds, minutes, longer, depending input. Wide variance large n often lead many samples, .e., long runtimes. effects interact dynamically. example, large n, even small increases sd can greatly increase runtime number values found. inputs inconsistent, solution. function return empty results throw warning.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate CLOSURE samples — closure_generate","text":"","code":"closure_generate(   mean,   sd,   n,   scale_min,   scale_max,   rounding = \"up_or_down\",   threshold = 5,   warn_if_empty = TRUE,   ask_to_proceed = TRUE,   rounding_error_mean = NULL,   rounding_error_sd = NULL )"},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate CLOSURE samples — closure_generate","text":"mean String (length 1). Reported mean. sd String (length 1). Reported sample standard deviation. n Numeric (length 1). Reported sample size. scale_min, scale_max Numeric (length 1 ). Minimal maximal possible values measurement scale. example, 1-7 Likert scale, use scale_min = 1 scale_max = 7. Prefer empirical min max available: constrain possible values . rounding String (length 1). Rounding method assumed created mean sd. See Rounding options, also Rounding limitations section . Default \"up_or_down\" , e.g., unrounds 0.12 0.115 lower bound 0.125 upper bound. threshold Numeric (length 1). Number round , rounding \"up_or_down\", \"\", \"\". Default 5. warn_if_empty Logical (length 1). warning shown samples found? Default TRUE. ask_to_proceed Logical (length 1). runtime predicted long, function prompt proceed abort interactive setting? Default TRUE. rounding_error_mean, rounding_error_sd Numeric (length 1 ). Option manually set rounding error around mean sd. meant development might removed future, users can ignore .","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate CLOSURE samples — closure_generate","text":"Named list four tibbles (data frames): inputs: Arguments function. metrics: samples_initial: integer. basis computing CLOSURE results, based scale range . See closure_count_initial(). samples_all: integer. Number samples. Equal number rows results. values_all: integer. Number individual values found. Equal n * samples_all. horns: double. Measure dispersion bounded scales; see horns(). horns_uniform: double. Value horns reconstructed sample uniformly distributed. frequency: value: integer. Scale values derived scale_min scale_max. f_average: Count scale values mean results sample. f_absolute: integer. Count individual scale values found results samples. f_relative: double. Values' share total values found. results: id: integer. Runs 1 samples_all. sample: list integer vectors. vectors length n. sample (distribution) individual scale values found CLOSURE.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"rounding-limitations","dir":"Reference","previous_headings":"","what":"Rounding limitations","title":"Generate CLOSURE samples — closure_generate","text":"rounding threshold arguments fully implemented. example, CLOSURE currently treats rounding bounds inclusive, even rounding specification imply otherwise. Many specifications two arguments make difference, likely lead empty results.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate CLOSURE samples — closure_generate","text":"","code":"# High spread often leads to many samples -- # here, 3682. data_high <- closure_generate(   mean = \"3.5\",   sd = \"1.7\",   n = 70,   scale_min = 1,   scale_max = 5 )  data_high #> $inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 3.5   1.7      70         1         5 up_or_down         5 #>  #> $metrics #> # A tibble: 1 × 5 #>   samples_initial samples_all values_all horns horns_uniform #>             <int>       <int>      <int> <dbl>         <dbl> #> 1              15        2492     174440 0.708           0.5 #>  #> $frequency #> # A tibble: 5 × 4 #>   value f_average f_absolute f_relative #>   <int>     <dbl>      <int>      <dbl> #> 1     1     16.4       40982     0.235  #> 2     2      7.19      17922     0.103  #> 3     3      5.29      13172     0.0755 #> 4     4      7.19      17916     0.103  #> 5     5     33.9       84448     0.484  #>  #> $results #> # A tibble: 2,492 × 2 #>       id sample     #>    <int> <list>     #>  1     1 <int [70]> #>  2     2 <int [70]> #>  3     3 <int [70]> #>  4     4 <int [70]> #>  5     5 <int [70]> #>  6     6 <int [70]> #>  7     7 <int [70]> #>  8     8 <int [70]> #>  9     9 <int [70]> #> 10    10 <int [70]> #> # ℹ 2,482 more rows #>   # Get a clear picture of the distribution # by following up with `closure_plot_bar()`: closure_plot_bar(data_high)   # Low spread, only 3 samples, and not all # scale values are possible. data_low <- closure_generate(   mean = \"2.9\",   sd = \"0.5\",   n = 70,   scale_min = 1,   scale_max = 5 )  data_low #> $inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 2.9   0.5      70         1         5 up_or_down         5 #>  #> $metrics #> # A tibble: 1 × 5 #>   samples_initial samples_all values_all  horns horns_uniform #>             <int>       <int>      <int>  <dbl>         <dbl> #> 1              15         219      15330 0.0643           0.5 #>  #> $frequency #> # A tibble: 5 × 4 #>   value f_average f_absolute f_relative #>   <int>     <dbl>      <int>      <dbl> #> 1     1     1.59         349    0.0228  #> 2     2     7.42        1626    0.106   #> 3     3    57.8        12659    0.826   #> 4     4     2.61         572    0.0373  #> 5     5     0.566        124    0.00809 #>  #> $results #> # A tibble: 219 × 2 #>       id sample     #>    <int> <list>     #>  1     1 <int [70]> #>  2     2 <int [70]> #>  3     3 <int [70]> #>  4     4 <int [70]> #>  5     5 <int [70]> #>  6     6 <int [70]> #>  7     7 <int [70]> #>  8     8 <int [70]> #>  9     9 <int [70]> #> 10    10 <int [70]> #> # ℹ 209 more rows #>   # This can also be shown by `closure_plot_bar()`: closure_plot_bar(data_low)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_horns_analyze.html","id":null,"dir":"Reference","previous_headings":"","what":"Horns index for each CLOSURE sample — closure_horns_analyze","title":"Horns index for each CLOSURE sample — closure_horns_analyze","text":"Following closure_generate(), can call closure_horns_analyze() compute horns index individual sample compute summary statistics distribution indices. See horns() metric . adds detail \"horns\" \"horns_uniform\" columns output closure_generate(), \"horns\" overall mean per-sample indices found . closure_horns_histogram() draws quick barplot reveal distribution horns values. scale fixed 0 1.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_horns_analyze.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Horns index for each CLOSURE sample — closure_horns_analyze","text":"","code":"closure_horns_analyze(data)  closure_horns_histogram(   data,   bar_alpha = 0.8,   bar_color = \"#5D3FD3\",   bar_binwidth = 0.0025,   text_size = 12 )"},{"path":"https://lhdjung.github.io/unsum/reference/closure_horns_analyze.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Horns index for each CLOSURE sample — closure_horns_analyze","text":"data closure_horns_analyze(), list returned closure_generate(). closure_horns_histogram(), list returned closure_horns_analyze(). bar_alpha Numeric (length 1). Opacity bars. Default 0.8. bar_color String (length 1). Color bars. Default \"#5D3FD3\", purple color. bar_binwidth Width bins divide x-axis, passed ggplot2::geom_histogram(). Default 0.0025. text_size Numeric. Base font size pt. Default 12.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_horns_analyze.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Horns index for each CLOSURE sample — closure_horns_analyze","text":"closure_horns_analyze() returns named list two tibbles (data frames): horns_metrics: Summary statistics distribution horns index values: mean, uniform: horns horns_uniform closure_generate()'s output. sd: double. Standard deviation. cv: double. Coefficient variation, .e., sd / mean. mad: double. Median absolute deviation; see stats::mad(). min, median, max: double. Minimum, median, maximum horns index. range: double. Equal max - min. horns_results: id: integer. Uniquely identifies horns index, just like corresponding samples closure_generate(). horns: double. Horns index individual sample. closure_horns_histogram() returns ggplot object.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_horns_analyze.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Horns index for each CLOSURE sample — closure_horns_analyze","text":"\"mad\" column overrides default stats::mad(): adjusting result via multiplication constant (1.48). assumes normal distribution, generally seem case horns index values. , constant set 1.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_horns_analyze.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Horns index for each CLOSURE sample — closure_horns_analyze","text":"","code":"data <- closure_generate(   mean = \"2.9\",   sd = \"0.5\",   n = 70,   scale_min = 1,   scale_max = 5 )  data_horns <- closure_horns_analyze(data) data_horns #> $closure_generate_inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 2.9   0.5      70         1         5 up_or_down         5 #>  #> $horns_metrics #> # A tibble: 1 × 9 #>     mean uniform      sd    cv     mad    min median    max  range #>    <dbl>   <dbl>   <dbl> <dbl>   <dbl>  <dbl>  <dbl>  <dbl>  <dbl> #> 1 0.0643     0.5 0.00690 0.107 0.00551 0.0511 0.0654 0.0737 0.0227 #>  #> $horns_results #> # A tibble: 219 × 2 #>       id  horns #>    <int>  <dbl> #>  1     1 0.0563 #>  2     2 0.0523 #>  3     3 0.0563 #>  4     4 0.0635 #>  5     5 0.0523 #>  6     6 0.0706 #>  7     7 0.0594 #>  8     8 0.0553 #>  9     9 0.0635 #> 10    10 0.0706 #> # ℹ 209 more rows #>   closure_horns_histogram(data_horns)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize CLOSURE data in a histogram — closure_plot_bar","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"Call closure_plot_bar() get barplot CLOSURE results. scale value, bars show often value appears full list possible raw data samples found CLOSURE algorithm.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"","code":"closure_plot_bar(   data,   frequency = c(\"absolute-percent\", \"absolute\", \"relative\", \"percent\"),   samples = c(\"mean\", \"all\"),   bar_alpha = 0.75,   bar_color = \"#5D3FD3\",   show_text = TRUE,   text_color = bar_color,   text_size = 12,   text_offset = 0.05,   mark_thousand = \",\",   mark_decimal = \".\" )"},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"data List returned closure_generate(). frequency String (length 1). bars display? default, \"absolute-percent\", displays count scale value percentage values. options \"absolute\", \"relative\", \"percent\". samples String (length 1). aggregate samples? Either take average sample (\"mean\", default) sum samples (\"\"). matters absolute frequencies shown. bar_alpha Numeric (length 1). Opacity bars. Default 0.75. bar_color String (length 1). Color bars. Default \"#5D3FD3\", purple color. show_text Logical (length 1). bars labeled corresponding frequencies? Default TRUE. text_color String (length 1). Color frequency labels. default, bar_color. text_size Numeric. Base font size pt. Default 12. text_offset Numeric (length 1). Distance text labels bars. Default 0.05. mark_thousand, mark_decimal Strings (length 1 ). Delimiters groups digits text labels. Defaults \",\" mark_thousand (e.g., \"20,000\") \".\" mark_decimal (e.g., \"0.15\").","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"ggplot object.","code":""},{"path":[]},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"","code":"# Create CLOSURE data first: data <- closure_generate(   mean = \"3.5\",   sd = \"2\",   n = 52,   scale_min = 1,   scale_max = 5 )  # Visualize: closure_plot_bar(data)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"Call closure_plot_ecdf() visualize CLOSURE results using data's empirical cumulative distribution function (ECDF). diagonal reference line benchmarks ECDF hypothetical linear relationship. See closure_plot_bar() intuitive visuals.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"","code":"closure_plot_ecdf(   data,   samples = c(\"mean\", \"all\"),   line_color = \"#5D3FD3\",   text_size = 12,   reference_line_alpha = 0.6,   pad = TRUE )"},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"data List returned closure_generate(). samples String (length 1). aggregate samples? Either draw single ECDF line average sample (\"mean\", default); draw separate line sample (\"\"). Note: latter option can slow many values found. line_color String (length 1). Color ECDF line. Default \"#5D3FD3\", purple color. text_size Numeric. Base font size pt. Default 12. reference_line_alpha Numeric (length 1). Opacity diagonal reference line. Default 0.6. pad Logical (length 1). ECDF line padded x-axis stretches beyond data points? Default TRUE.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"ggplot object.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"present function inspired rsprite2::plot_distributions(). However, plot_distributions() shows multiple lines based SPRITE, draws random samples possible datasets. CLOSURE exhaustive, closure_plot_ecdf() shows possible datasets single line default.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"","code":"# Create CLOSURE data first: data <- closure_generate(   mean = \"3.5\",   sd = \"2\",   n = 52,   scale_min = 1,   scale_max = 5 )  # Visualize: closure_plot_ecdf(data)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Write CLOSURE results to disk (and read them back in) — closure_write","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"can use closure_write() save results closure_generate() computer. message show exact location. data saved new folder four separate files, one tibble closure_generate()'s output. folder named parameters closure_generate(). closure_read() opposite: reads files back R, recreating original CLOSURE list. useful later analyses want re-run lengthy closure_generate() call.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"","code":"closure_write(data, path)  closure_read(path)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"data List returned closure_generate(). path String (length 1). File path closure_write() create new folder results. Set \".\" choose current working directory. closure_read(), path existing folder results.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"closure_write() returns path new folder created, closure_read() returns list.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"closure_write() saves first three tibbles CSVs, \"results\" tibble becomes Parquet file. much faster takes far less disk space — roughly 1% CSV file data. Speed disk space can relevant large result sets. Use closure_read() recreate CLOSURE list folder. One reasons convenient opening Parquet file requires special reader. general tool, see nanoparquet::read_parquet().","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"folder-name","dir":"Reference","previous_headings":"","what":"Folder name","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"new folder's name sufficient recreate CLOSURE results. Dashes separate values, underscores replace decimal periods. example: order closure_generate():","code":"CLOSURE-3_5-1_0-90-1-5-up_or_down-5 closure_generate(     mean = \"3.5\",     sd = \"1.0\",     n = 90,     scale_min = 1,     scale_max = 5,     rounding = \"up_or_down\",  # default     threshold = 5             # default   )"},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"","code":"if (interactive()) {   data <- closure_generate(     mean = \"2.7\",     sd = \"0.6\",     n = 45,     scale_min = 1,     scale_max = 5   )    # You should write to a real folder instead;   # or just leave `path` unspecified. I use a   # fake folder just for this example.   path_new_folder <- closure_write(data, path = tempdir())    # In a later session, conveniently read the files   # back into R. This returns the original list,   # identical except for floating-point error.   closure_read(path_new_folder) }"},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":null,"dir":"Reference","previous_headings":"","what":"Horns index (\\(h\\)) — horns","title":"Horns index (\\(h\\)) — horns","text":"horns() measures dispersion sample clamped observations based scale limits. ranges 0 1: 0 means variation, .e., observations value. 1 means observations evenly split extremes, none . horns_uniform() computes value horns() return uniform distribution within given scale limits. can useful point reference horns(). two functions create horns horns_uniform columns closure_generate(). horns_rescaled() version horns()  normalized scale length, 0.5 always indicates uniform distribution, independent number scale points. meant enable comparison across scales different lengths, harder interpret individual scale. Even , range meaning 0 1 horns().","code":""},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Horns index (\\(h\\)) — horns","text":"","code":"horns(freqs, scale_min, scale_max)  horns_uniform(scale_min, scale_max)  horns_rescaled(freqs, scale_min, scale_max)"},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Horns index (\\(h\\)) — horns","text":"freqs Numeric. Vector frequencies (relative absolute) binned observations; e.g., vector 5 elements 1-5 scale. scale_min, scale_max Numeric (length 1 ). Minimum maximum scale values measured. can lower upper bounds (e.g., 1-5 Likert scale) empirical min max reported article. latter preferred available constrain scale .","code":""},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Horns index (\\(h\\)) — horns","text":"Numeric (length 1).","code":""},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Horns index (\\(h\\)) — horns","text":"horns index \\(h\\) defined : $$      h = \\frac      {\\sum_{=1}^{k} f_i (s_i - \\bar{s})^2}      {\\frac{1}{4} (s_{\\max} - s_{\\min})^2}   $$ \\(k\\) number scale points (.e., length freqs ), \\(f_i\\) relative frequency \\(\\)th scale point, \\(s_i\\); \\(\\bar{s}\\) sample mean, \\(s_{\\max}\\) upper bound scale, \\(s_{\\min}\\) lower bound. name inspired Heathers (2017a) defines \"horns confidence\" reconstructed sample \"incorrect, impossible unlikely value set constituents stacked highest lowest bins try meet ludicrously high SD\". purest form, case horns() returns 1. However, note implications plausibility given set summary statistics depend substantive context data (Heathers 2017b).","code":""},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Horns index (\\(h\\)) — horns","text":"","code":"# For simplicity, all examples use a 1-5 scale and a total N of 300.  # ---- With all values at the extremes  horns(freqs = c(300, 0, 0, 0, 0), scale_min = 1, scale_max = 5) #> [1] 0  horns(c(150, 0, 0, 0, 150), 1, 5) #> [1] 1  horns(c(100, 0, 0, 0, 200), 1, 5) #> [1] 0.8888889   # ---- With some values in between  horns(c(60, 60, 60, 60, 60), 1, 5) #> [1] 0.5  horns(c(200, 50, 30, 20, 0), 1, 5) #> [1] 0.2113889  horns(c(150, 100, 50, 0, 0), 1, 5) #> [1] 0.1388889  horns(c(100, 40, 20, 40, 100), 1, 5) #> [1] 0.7333333"},{"path":"https://lhdjung.github.io/unsum/news/index.html","id":"unsum-020","dir":"Changelog","previous_headings":"","what":"unsum 0.2.0","title":"unsum 0.2.0","text":"Initial CRAN submission. Added closure_horns_analyze() closure_horns_histogram(). Removed vignette installing Rust since users need package CRAN. Fixed examples causes CRAN check issues.","code":""}]
