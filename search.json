[{"path":"https://lhdjung.github.io/unsum/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 unsum authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://lhdjung.github.io/unsum/articles/install-rust.html","id":"windows","dir":"Articles","previous_headings":"","what":"Windows","title":"Installing Rust for unsum","text":"Go Install Rust page, download rustup-init.exe (32-bit 64-bit?), run PC. open terminal. given choice 1, 2, 3, hit 1, Enter . Wait Visual Studio install. can take . finished, hit Enter terminal close Visual Studio. Rust installation finished, hit Enter close terminal. Open new terminal (?), paste command rustup target add x86_64-pc-windows-gnu using Ctrl+V, hit Enter.","code":""},{"path":"https://lhdjung.github.io/unsum/articles/install-rust.html","id":"mac","dir":"Articles","previous_headings":"","what":"Mac","title":"Installing Rust for unsum","text":"Open terminal (?). Go Install Rust page, copy line starts curl, paste terminal using Command+V, hit Enter. given choice 1, 2, 3, hit 1, Enter .","code":""},{"path":"https://lhdjung.github.io/unsum/articles/install-rust.html","id":"linux","dir":"Articles","previous_headings":"","what":"Linux","title":"Installing Rust for unsum","text":"See instructions Mac. Spot differences . know .","code":""},{"path":"https://lhdjung.github.io/unsum/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Lukas Jung. Author, maintainer. Nathanael Larigaldie. Contributor.","code":""},{"path":"https://lhdjung.github.io/unsum/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Jung L (2025). unsum: Reconstruct Raw Data Summary Statistics. R package version 0.1.0, https://github.com/lhdjung/unsum.","code":"@Manual{,   title = {unsum: Reconstruct Raw Data from Summary Statistics},   author = {Lukas Jung},   year = {2025},   note = {R package version 0.1.0},   url = {https://github.com/lhdjung/unsum}, }"},{"path":"https://lhdjung.github.io/unsum/index.html","id":"unsum-reconstruct-raw-data-from-summary-statistics","dir":"","previous_headings":"","what":"Reconstruct Raw Data from Summary Statistics","title":"Reconstruct Raw Data from Summary Statistics","text":"goal unsum undo summarization: reconstruct possible samples may underlie given set summary statistics. currently supports sets mean, SD, sample size, scale bounds. can useful forensic metascience identify impossible implausible reported numbers. package features CLOSURE: Complete Listing Original Samples Underlying Raw Evidence, fast algorithm implemented Rust. Go Get started learn use . CLOSURE exhaustive, makes computationally intensive. code takes long run, consider using SPRITE instead (see Previous work ).","code":""},{"path":"https://lhdjung.github.io/unsum/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Reconstruct Raw Data from Summary Statistics","text":"can install development version unsum GitHub either : R version 4.2.0 recent. run unsum, also need Rust installation; see Installing Rust unsum.","code":"remotes::install_github(\"lhdjung/unsum\") # or pak::pak(\"lhdjung/unsum\")"},{"path":"https://lhdjung.github.io/unsum/index.html","id":"demo","dir":"","previous_headings":"","what":"Demo","title":"Reconstruct Raw Data from Summary Statistics","text":"Start closure_generate(), package’s main function. creates possible samples: Visualize overall distribution values found samples:","code":"library(unsum)  data <- closure_generate(   mean = \"2.7\",   sd = \"1.9\",   n = 130,   scale_min = 1,   scale_max = 5 ) #> → Just a second...  data #> $inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 2.7   1.9     130         1         5 up_or_down         5 #>  #> $metrics #> # A tibble: 1 × 5 #>   samples_initial samples_all values_all horns horns_uniform #>             <int>       <int>      <int> <dbl>         <dbl> #> 1              15        5359     696670 0.881           0.5 #>  #> $frequency #> # A tibble: 5 × 4 #>   value f_average f_absolute f_relative #>   <int>     <dbl>      <int>      <dbl> #> 1     1     67.0      358972     0.515  #> 2     2      5.63      30170     0.0433 #> 3     3      4.09      21940     0.0315 #> 4     4      5.63      30162     0.0433 #> 5     5     47.7      255426     0.367  #>  #> $results #> # A tibble: 5,359 × 2 #>       id sample      #>    <int> <list>      #>  1     1 <int [130]> #>  2     2 <int [130]> #>  3     3 <int [130]> #>  4     4 <int [130]> #>  5     5 <int [130]> #>  6     6 <int [130]> #>  7     7 <int [130]> #>  8     8 <int [130]> #>  9     9 <int [130]> #> 10    10 <int [130]> #> # ℹ 5,349 more rows closure_plot_bar(data)"},{"path":"https://lhdjung.github.io/unsum/index.html","id":"previous-work","dir":"","previous_headings":"","what":"Previous work","title":"Reconstruct Raw Data from Summary Statistics","text":"SPRITE generates random datasets led reported statistics. CLOSURE exhaustive, always finds possible datasets, just random sample . reason, SPRITE runs fast CLOSURE may take long. GRIM GRIMMER test reported summary statistics consistency, CLOSURE ultimate consistency test: finds least one distribution, statistics consistent; , correct. CORVIDS deserves credit first technique reconstruct possible underlying datasets. However, takes long run, often prohibitively . partly code written Python, algorithm also inherently much complex CLOSURE.","code":""},{"path":"https://lhdjung.github.io/unsum/index.html","id":"about","dir":"","previous_headings":"","what":"About","title":"Reconstruct Raw Data from Summary Statistics","text":"CLOSURE algorithm originally written Python Nathanael Larigaldie. R package unsum provides easy access optimized implementation Rust, closure-core, via amazing extendr framework. Rust code tends run much faster R Python code, required many applications CLOSURE.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":null,"dir":"Reference","previous_headings":"","what":"Count CLOSURE samples in advance — closure_count_initial","title":"Count CLOSURE samples in advance — closure_count_initial","text":"Determine many samples closure_generate() find given set summary statistics. closure_count_initial() counts first round samples, ones generated. currently closure_count_all() function. can help predict much time closure_generate() take, avoid prohibitively long runs.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Count CLOSURE samples in advance — closure_count_initial","text":"","code":"closure_count_initial(scale_min, scale_max)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Count CLOSURE samples in advance — closure_count_initial","text":"scale_min, scale_max Integers (length 1 ). Minimum maximum scales reported statistics refer.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Count CLOSURE samples in advance — closure_count_initial","text":"Integer (length 1).","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_count_initial.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Count CLOSURE samples in advance — closure_count_initial","text":"","code":"closure_count_initial(scale_min = 1, scale_max = 5) #> [1] 15"},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":null,"dir":"Reference","previous_headings":"","what":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"run closure_generate(), may want get sense time take run. Use closure_gauge_complexity() compute heuristics-based complexity score. reference, determines messages closure_generate():","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"","code":"closure_gauge_complexity(mean, sd, n, scale_min, scale_max)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"mean String (length 1). Reported mean. sd String (length 1). Reported sample standard deviation. n Numeric (length 1). Reported sample size. scale_min, scale_max Numeric (length 1 ). Minimal maximal possible values measurement scale. example, 1-7 Likert scale, use scale_min = 1 scale_max = 7. Prefer empirical min max available: constrain possible values .","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"Numeric (length 1).","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"result function hard interpret. can convey idea likely runtime CLOSURE. input parameters interact highly dynamic ways, makes prediction difficult. addition, even progress bars updates regular intervals (e.g., \"10% complete\") prove extremely challenging: Rust code computes CLOSURE results parallel, makes hard get overview total progress across cores; especially display information R level.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_gauge_complexity.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Heuristic to predict CLOSURE runtime — closure_gauge_complexity","text":"","code":"# Low SD, N, and scale range: closure_gauge_complexity(   mean = 2.55,   sd = 0.85,   n = 84,   scale_min = 1,   scale_max = 5 ) #> [1] 0.3521825  # Somewhat higher: closure_gauge_complexity(   mean = 4.26,   sd = 1.58,   n = 100,   scale_min = 1,   scale_max = 7 ) #> [1] 2.477121  # Very high: closure_gauge_complexity(   mean = 3.81,   sd = 3.09,   n = 156,   scale_min = 1,   scale_max = 7 ) #> [1] 3.39794"},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":null,"dir":"Reference","previous_headings":"","what":"Generate CLOSURE samples — closure_generate","title":"Generate CLOSURE samples — closure_generate","text":"Call closure_generate() run CLOSURE algorithm given set summary statistics. can take seconds, minutes, longer, depending input. Wide variance large n often lead many samples, .e., long runtimes. effects interact dynamically. example, large n, even small increases sd can greatly increase runtime number values found. inputs inconsistent, solution. function return empty results throw warning.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate CLOSURE samples — closure_generate","text":"","code":"closure_generate(   mean,   sd,   n,   scale_min,   scale_max,   rounding = \"up_or_down\",   threshold = 5,   warn_if_empty = TRUE,   ask_to_proceed = TRUE,   rounding_error_mean = NULL,   rounding_error_sd = NULL )"},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate CLOSURE samples — closure_generate","text":"mean String (length 1). Reported mean. sd String (length 1). Reported sample standard deviation. n Numeric (length 1). Reported sample size. scale_min, scale_max Numeric (length 1 ). Minimal maximal possible values measurement scale. example, 1-7 Likert scale, use scale_min = 1 scale_max = 7. Prefer empirical min max available: constrain possible values . rounding String (length 1). Rounding method assumed created mean sd. See Rounding options, also Rounding limitations section . Default \"up_or_down\" , e.g., unrounds 0.12 0.115 lower bound 0.125 upper bound. threshold Numeric (length 1). Number round , rounding \"up_or_down\", \"\", \"\". Default 5. warn_if_empty Logical (length 1). warning shown samples found? Default TRUE. ask_to_proceed Logical (length 1). runtime predicted long, function prompt proceed abort interactive setting? Default TRUE. rounding_error_mean, rounding_error_sd Numeric (length 1 ). Option manually set rounding error around mean sd. meant development might removed future, users can ignore .","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate CLOSURE samples — closure_generate","text":"Named list four tibbles (data frames): inputs: Arguments function. metrics: samples_initial: integer. basis computing CLOSURE results, based scale range . See closure_count_initial(). samples_all: integer. Number samples. Equal number rows results. values_all: integer. Number individual values found. Equal n * samples_all. horns: double. Measure dispersion bounded scales; see horns(). horns_uniform: double. Value horns reconstructed sample uniformly distributed. frequency: value: integer. Scale values derived scale_min scale_max. f_average: Count scale values mean results sample. f_absolute: integer. Count individual scale values found results samples. f_relative: double. Values' share total values found. results: id: integer. Runs 1 samples_all. sample: list integer vectors. vectors length n. sample (distribution) individual scale values found CLOSURE.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"rounding-limitations","dir":"Reference","previous_headings":"","what":"Rounding limitations","title":"Generate CLOSURE samples — closure_generate","text":"rounding threshold arguments fully implemented. example, CLOSURE currently treats rounding bounds inclusive, even rounding specification imply otherwise. Many specifications two arguments make difference, likely lead empty results.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_generate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate CLOSURE samples — closure_generate","text":"","code":"# High spread often leads to many samples -- # here, 3682. data_high <- closure_generate(   mean = \"3.5\",   sd = \"1.7\",   n = 70,   scale_min = 1,   scale_max = 5 )  data_high #> $inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 3.5   1.7      70         1         5 up_or_down         5 #>  #> $metrics #> # A tibble: 1 × 5 #>   samples_initial samples_all values_all horns horns_uniform #>             <int>       <int>      <int> <dbl>         <dbl> #> 1              15        2492     174440 0.708           0.5 #>  #> $frequency #> # A tibble: 5 × 4 #>   value f_average f_absolute f_relative #>   <int>     <dbl>      <int>      <dbl> #> 1     1     16.4       40982     0.235  #> 2     2      7.19      17922     0.103  #> 3     3      5.29      13172     0.0755 #> 4     4      7.19      17916     0.103  #> 5     5     33.9       84448     0.484  #>  #> $results #> # A tibble: 2,492 × 2 #>       id sample     #>    <int> <list>     #>  1     1 <int [70]> #>  2     2 <int [70]> #>  3     3 <int [70]> #>  4     4 <int [70]> #>  5     5 <int [70]> #>  6     6 <int [70]> #>  7     7 <int [70]> #>  8     8 <int [70]> #>  9     9 <int [70]> #> 10    10 <int [70]> #> # ℹ 2,482 more rows #>   # Get a clear picture of the distribution # by following up with `closure_plot_bar()`: closure_plot_bar(data_high)   # Low spread, only 3 samples, and not all # scale values are possible. data_low <- closure_generate(   mean = \"2.9\",   sd = \"0.5\",   n = 70,   scale_min = 1,   scale_max = 5 )  data_low #> $inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 2.9   0.5      70         1         5 up_or_down         5 #>  #> $metrics #> # A tibble: 1 × 5 #>   samples_initial samples_all values_all  horns horns_uniform #>             <int>       <int>      <int>  <dbl>         <dbl> #> 1              15         219      15330 0.0643           0.5 #>  #> $frequency #> # A tibble: 5 × 4 #>   value f_average f_absolute f_relative #>   <int>     <dbl>      <int>      <dbl> #> 1     1     1.59         349    0.0228  #> 2     2     7.42        1626    0.106   #> 3     3    57.8        12659    0.826   #> 4     4     2.61         572    0.0373  #> 5     5     0.566        124    0.00809 #>  #> $results #> # A tibble: 219 × 2 #>       id sample     #>    <int> <list>     #>  1     1 <int [70]> #>  2     2 <int [70]> #>  3     3 <int [70]> #>  4     4 <int [70]> #>  5     5 <int [70]> #>  6     6 <int [70]> #>  7     7 <int [70]> #>  8     8 <int [70]> #>  9     9 <int [70]> #> 10    10 <int [70]> #> # ℹ 209 more rows #>   # This can also be shown by `closure_plot_bar()`: closure_plot_bar(data_low)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize CLOSURE data in a histogram — closure_plot_bar","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"Call closure_plot_bar() get barplot CLOSURE results. scale value, bars show often value appears full list possible raw data samples found CLOSURE algorithm.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"","code":"closure_plot_bar(   data,   frequency = c(\"absolute-percent\", \"absolute\", \"relative\", \"percent\"),   samples = c(\"mean\", \"all\"),   bar_alpha = 0.75,   bar_color = \"#5D3FD3\",   show_text = TRUE,   text_color = bar_color,   text_size = 12,   text_offset = 0.05,   mark_thousand = \",\",   mark_decimal = \".\" )"},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"data List returned closure_generate(). frequency String (length 1). bars display? default, \"absolute-percent\", displays count scale value percentage values. options \"absolute\", \"relative\", \"percent\". samples String (length 1). aggregate samples? Either take average sample (\"mean\", default) sum samples (\"\"). matters absolute frequencies shown. bar_alpha Numeric (length 1). Opacity bars. Default 0.75. bar_color String (length 1). Color bars. Default \"#5D3FD3\", purple color. show_text Logical (length 1). bars labeled corresponding frequencies? Default TRUE. text_color String (length 1). Color frequency labels. default, bar_color. text_size Numeric. Base font size pt. Default 12. text_offset Numeric (length 1). Distance text labels bars. Default 0.05. mark_thousand, mark_decimal Strings (length 1 ). Delimiters groups digits text labels. Defaults \",\" mark_thousand (e.g., \"20,000\") \".\" mark_decimal (e.g., \"0.15\").","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"ggplot object.","code":""},{"path":[]},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_bar.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize CLOSURE data in a histogram — closure_plot_bar","text":"","code":"# Create CLOSURE data first: data <- closure_generate(   mean = \"3.5\",   sd = \"2\",   n = 52,   scale_min = 1,   scale_max = 5 )  # Visualize: closure_plot_bar(data)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":null,"dir":"Reference","previous_headings":"","what":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"Call closure_plot_ecdf() visualize CLOSURE results using data's empirical cumulative distribution function (ECDF). diagonal reference line benchmarks ECDF hypothetical linear relationship. See closure_plot_bar() intuitive visuals.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"","code":"closure_plot_ecdf(   data,   samples = c(\"mean\", \"all\"),   line_color = \"#5D3FD3\",   text_size = 12,   reference_line_alpha = 0.6,   pad = TRUE )"},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"data List returned closure_generate(). samples String (length 1). aggregate samples? Either draw single ECDF line average sample (\"mean\", default); draw separate line sample (\"\"). Note: latter option can slow many values found. line_color String (length 1). Color ECDF line. Default \"#5D3FD3\", purple color. text_size Numeric. Base font size pt. Default 12. reference_line_alpha Numeric (length 1). Opacity diagonal reference line. Default 0.6. pad Logical (length 1). ECDF line padded x-axis stretches beyond data points? Default TRUE.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"ggplot object.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"present function inspired rsprite2::plot_distributions(). However, plot_distributions() shows multiple lines based SPRITE, draws random samples possible datasets. CLOSURE exhaustive, closure_plot_ecdf() shows possible datasets single line default.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_plot_ecdf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Visualize CLOSURE data in an ECDF plot — closure_plot_ecdf","text":"","code":"# Create CLOSURE data first: data <- closure_generate(   mean = \"3.5\",   sd = \"2\",   n = 52,   scale_min = 1,   scale_max = 5 )  # Visualize: closure_plot_ecdf(data)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":null,"dir":"Reference","previous_headings":"","what":"Write CLOSURE results to disk (and read them back in) — closure_write","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"can use closure_write() save results closure_generate() computer. message show exact location. data saved new folder four separate files, one tibble closure_generate()'s output. folder named parameters closure_generate(). closure_read() opposite: reads files back R, recreating original CLOSURE list. useful later analyses want re-run lengthy closure_generate() call.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"","code":"closure_write(data, path = \".\")  closure_read(path)"},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"data List returned closure_generate(). path String (length 1). File path closure_write() create new folder results. default, current working directory. closure_read(), path new folder.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"closure_write() returns path new folder created, closure_read() returns list.","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"closure_write() saves first three tibbles CSVs, \"results\" tibble becomes Parquet file. much faster takes far less disk space — roughly 1% CSV file data. Speed disk space can relevant large result sets. Use closure_read() recreate CLOSURE list folder. One reasons convenient opening Parquet file requires special reader. general tool, see nanoparquet::read_parquet().","code":""},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"folder-name","dir":"Reference","previous_headings":"","what":"Folder name","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"new folder's name sufficient recreate CLOSURE results. Dashes separate values, underscores replace decimal periods. example: order closure_generate():","code":"CLOSURE-3_5-1_0-90-1-5-up_or_down-5 closure_generate(     mean = \"3.5\",     sd = \"1.0\",     n = 90,     scale_min = 1,     scale_max = 5,     rounding = \"up_or_down\",  # default     threshold = 5             # default   )"},{"path":"https://lhdjung.github.io/unsum/reference/closure_write.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Write CLOSURE results to disk (and read them back in) — closure_write","text":"","code":"data <- closure_generate(   mean = \"2.7\",   sd = \"0.6\",   n = 45,   scale_min = 1,   scale_max = 5 )  # Just for this example -- don't try at home fake_folder <- tempdir()  # You should write to a real folder instead; # or just leave `path` unspecified. path_new_folder <- closure_write(data, path = fake_folder) #> ✔ All files written to: #> /tmp/Rtmptr0TYZ/CLOSURE-2_7-0_6-45-1-5-up_or_down-5/  # In a later session, conveniently read the files # back into R. This returns the original list, # identical except for floating-point error. closure_read(path_new_folder) #> $inputs #> # A tibble: 1 × 7 #>   mean  sd        n scale_min scale_max rounding   threshold #>   <chr> <chr> <dbl>     <dbl>     <dbl> <chr>          <dbl> #> 1 2.7   0.6      45         1         5 up_or_down         5 #>  #> $metrics #> # A tibble: 1 × 5 #>   samples_initial samples_all values_all  horns horns_uniform #>             <int>       <int>      <int>  <dbl>         <dbl> #> 1              15          54       2430 0.0906           0.5 #>  #> $frequency #> # A tibble: 5 × 4 #>   value f_average f_absolute f_relative #>   <int>     <dbl>      <int>      <dbl> #> 1     1     1.37          74    0.0305  #> 2     2    12.7          685    0.282   #> 3     3    29.4         1585    0.652   #> 4     4     1.37          74    0.0305  #> 5     5     0.222         12    0.00494 #>  #> $results #> # A tibble: 54 × 2 #>       id sample     #>    <int> <list>     #>  1     1 <int [45]> #>  2     2 <int [45]> #>  3     3 <int [45]> #>  4     4 <int [45]> #>  5     5 <int [45]> #>  6     6 <int [45]> #>  7     7 <int [45]> #>  8     8 <int [45]> #>  9     9 <int [45]> #> 10    10 <int [45]> #> # ℹ 44 more rows #>"},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":null,"dir":"Reference","previous_headings":"","what":"Horns index (\\(h\\)) — horns","title":"Horns index (\\(h\\)) — horns","text":"horns() measures dispersion sample clamped observations based scale limits. ranges 0 1: 0 means variation, .e., observations value. 1 means observations evenly split extremes, none . horns_uniform() computes value horns() return uniform distribution within given scale limits. can useful point reference horns(). two functions create horns horns_uniform columns closure_generate(). horns_rescaled() version horns()  normalized scale length, 0.5 always indicates uniform distribution, independent number scale points. meant enable comparison across scales different lengths, harder interpret individual scale. Even , range meaning 0 1 horns().","code":""},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Horns index (\\(h\\)) — horns","text":"","code":"horns(freqs, scale_min, scale_max)  horns_uniform(scale_min, scale_max)  horns_rescaled(freqs, scale_min, scale_max)"},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Horns index (\\(h\\)) — horns","text":"freqs Numeric. Vector frequencies (relative absolute) binned observations; e.g., vector 5 elements 1-5 scale. scale_min, scale_max Numeric (length 1 ). Minimum maximum scale values measured. can lower upper bounds (e.g., 1-5 Likert scale) empirical min max reported article. latter preferred available constrain scale .","code":""},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Horns index (\\(h\\)) — horns","text":"Numeric (length 1).","code":""},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Horns index (\\(h\\)) — horns","text":"horns index \\(h\\) defined : $$      h = \\frac      {\\sum_{=1}^{k} f_i (s_i - \\bar{s})^2}      {\\frac{1}{4} (s_{\\max} - s_{\\min})^2}   $$ \\(k\\) number scale points (.e., length freqs ), \\(f_i\\) relative frequency \\(\\)th scale point, \\(s_i\\); \\(\\bar{s}\\) sample mean, \\(s_{\\max}\\) upper bound scale, \\(s_{\\min}\\) lower bound. name inspired Heathers (2017a) defines \"horns confidence\" reconstructed sample \"incorrect, impossible unlikely value set constituents stacked highest lowest bins try meet ludicrously high SD\". purest form, case horns() returns 1. However, note implications plausibility given set summary statistics depend substantive context data (Heathers 2017b).","code":""},{"path":"https://lhdjung.github.io/unsum/reference/horns.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Horns index (\\(h\\)) — horns","text":"","code":"# For simplicity, all examples use a 1-5 scale and a total N of 300.  # ---- With all values at the extremes  horns(freqs = c(300, 0, 0, 0, 0), scale_min = 1, scale_max = 5) #> [1] 0  horns(c(150, 0, 0, 0, 150), 1, 5) #> [1] 1  horns(c(100, 0, 0, 0, 200), 1, 5) #> [1] 0.8888889   # ---- With some values in between  horns(c(60, 60, 60, 60, 60), 1, 5) #> [1] 0.5  horns(c(200, 50, 30, 20, 0), 1, 5) #> [1] 0.2113889  horns(c(150, 100, 50, 0, 0), 1, 5) #> [1] 0.1388889  horns(c(100, 40, 20, 40, 100), 1, 5) #> [1] 0.7333333"}]
