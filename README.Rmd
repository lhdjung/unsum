---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```


# unsum: reconstruct raw data from summary statistics

<!-- badges: start -->

<!-- badges: end -->

The goal of unsum is to **un**do **sum**marization: reconstruct all possible samples that may underlie a given set of summary statistics. It currently supports sets of mean, SD, sample size, and scale bounds. This can be useful in forensic metascience to identify impossible or implausible reported numbers.

The package features *CLOSURE: Complete Listing of Original Samples of Underlying Raw Evidence*, a fast algorithm implemented in Rust. Go to [*Get started*](https://lhdjung.github.io/unsum/articles/unsum.html) to learn how to use it.

CLOSURE is exhaustive, which makes it computationally intensive. If your code takes too long to run, consider using [SPRITE](https://lukaswallrich.github.io/rsprite2//) instead (see *Previous work* below).

## Installation

You can install unsum with either of these:

``` r
install.packages("unsum")
# or
pak::pkg_install("unsum")
```

Your R version should be 4.2.0 or more recent.

## Demo

Start with `closure_generate()`, the package's main function. It creates all possible samples:

```{r example}
library(unsum)

data <- closure_generate(
  mean = "2.7",
  sd = "1.9",
  n = 130,
  scale_min = 1,
  scale_max = 5
)

data
```

Visualize the overall distribution of values found in the samples:

```{r}
#| fig.alt: >
#|   Barplot of `data`, the CLOSURE output.
#|   It specifically visualizes the `f_average` column of
#|   the `frequency` tibble, but also gives percentage figures,
#|   similar to the `f_relative` column. The overall shape is
#|   a somewhat polarized distribution.
closure_plot_bar(data)
```

## Previous work

[SPRITE](https://lukaswallrich.github.io/rsprite2/) generates random datasets that could have led to the reported statistics. CLOSURE is exhaustive, so it always finds all possible datasets, not just a random sample of them. For the same reason, SPRITE runs fast when CLOSURE may take too long.

[GRIM and GRIMMER](https://lhdjung.github.io/scrutiny/) test reported summary statistics for consistency, but CLOSURE is the ultimate consistency test: if it finds at least one distribution, the statistics are consistent; and if not, they cannot all be correct.

[CORVIDS](https://github.com/katherinemwood/corvids) deserves credit as the first technique to reconstruct all possible underlying datasets. However, it takes very long to run, often prohibitively so. This is partly because the code is written in Python, but the algorithm is also inherently much more complex than CLOSURE.

## About

The CLOSURE algorithm was originally written [in Python](https://github.com/larigaldie-n/CLOSURE-Python) by Nathanael Larigaldie. The R package unsum provides easy access to an optimized implementation in Rust, [closure-core](https://github.com/lhdjung/closure-core), via the amazing [extendr](https://extendr.github.io/) framework. Rust code tends to run much faster than R or Python code, which is required for many applications of CLOSURE.
